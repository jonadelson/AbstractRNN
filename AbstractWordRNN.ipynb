{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'abstracts.csv'\n",
    "\n",
    "def read_and_clean_data():\n",
    "    '''\n",
    "    Function to read and clean abstract data\n",
    "    '''\n",
    "    out = []\n",
    "    with open(file_name) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            abstract = line.strip()\n",
    "            \n",
    "            # The first line is junk until the word During\n",
    "            if i == 0:\n",
    "                abstract = abstract[abstract.find('During'):-1]\n",
    "                \n",
    "            # There are quotes in the data file, remove them\n",
    "            if abstract[0] == '\"' and abstract[-1] == '\"':\n",
    "                abstract = abstract[1:-1]\n",
    "                \n",
    "            out.append(unicode(abstract, 'utf-8'))\n",
    "\n",
    "    # The last line is junk so just return all but that\n",
    "    return out[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.punkt.PunktSentenceTokenizer()\n",
    "tokenize = nltk.word_tokenize \n",
    "abstracts = read_and_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_words():\n",
    "    for abstract in abstracts:\n",
    "        out = []\n",
    "        for sentence in sentence_tokenizer.tokenize(abstract):\n",
    "            for word in ['<START>'] + tokenize(sentence) + ['</START>']:\n",
    "                out.append(word)\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_word_set():\n",
    "    word_set = set()\n",
    "    for word_list in get_abstract_words():\n",
    "        for word in word_list:\n",
    "            if word in ['<START>', '</START>']:\n",
    "                word_set.add(word)\n",
    "            else:\n",
    "                word_set.add(word.lower())\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vector(line):\n",
    "    \"\"\"\n",
    "    Read in one word vector from the file\n",
    "    Each line comes in as a word followed by the \n",
    "    300 dimensional vector where each coordinate is\n",
    "    separated by a space\n",
    "    \"\"\"\n",
    "    split_line = line.split()\n",
    "    word, vector = split_line[0], split_line[1:]\n",
    "    vector = np.asarray([float(num) for num in vector], dtype='float32')\n",
    "\n",
    "    return word, vector\n",
    "\n",
    "def read_glove_vectors(word_set, file_name):\n",
    "    \"\"\"\n",
    "    Read in words from the file and yield the word/vector\n",
    "    if they are in the word set\n",
    "    \"\"\"\n",
    "    for word_vector in open(file_name):\n",
    "        word, vector = read_glove_vector(word_vector)\n",
    "        if word in word_set:\n",
    "            yield word, vector\n",
    "\n",
    "def get_all_glove_words(file_name):\n",
    "    word_set = set()\n",
    "    for word_vector in open(file_name):\n",
    "        word, _ = read_glove_vector(word_vector)\n",
    "        word_set.add(word)\n",
    "    return word_set\n",
    "\n",
    "def get_words_to_keep(file_name, min_count = 20):\n",
    "\n",
    "    # Collect all glove words\n",
    "    glove_set = get_all_glove_words(file_name)\n",
    "\n",
    "    # Get counts of words not in glove set\n",
    "    \n",
    "    word_set = set() # Final set of words to be used\n",
    "    unknown_count = Counter() # Get counts so we know what to include\n",
    "    for word_list in get_abstract_words():\n",
    "        for word in word_list:\n",
    "            # If we see the word in glove add it\n",
    "            # Otherwise get count of unknown words so we know which to keep\n",
    "            # Keep those above min_count\n",
    "            if word.lower() in glove_set:\n",
    "                word_set.add(word.lower())\n",
    "            elif word in ['<START>', '</START>']:\n",
    "                unknown_count[word] += 1\n",
    "            else:\n",
    "                unknown_count[word.lower()] += 1\n",
    "\n",
    "    # Keep only words greater than min_count\n",
    "    unknown_estimate = set(word_pair[0] for word_pair in unknown_count.iteritems() if word_pair[1] >= min_count)\n",
    "    unknown_ignore = set(word_pair[0] for word_pair in unknown_count.iteritems() if word_pair[1] < min_count)\n",
    "\n",
    "    word_set.update(unknown_estimate)\n",
    "    return word_set, unknown_estimate, unknown_ignore\n",
    "\n",
    "def create_word_embedding_matrix(file_name, min_count=20, dimension = 300):\n",
    "\n",
    "    unknown_keep.add('UNKNOWN_WORD')\n",
    "    word_set.add('UNKNOWN_WORD')\n",
    "\n",
    "    word2index = {w:i for i,w in enumerate(word_set)}\n",
    "    index2word = {i:w for i,w in enumerate(word_set)}\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word2index), dimension))\n",
    "\n",
    "    for word, vector in read_glove_vectors(word_set, file_name):\n",
    "        embedding_matrix[word2index[word],:] = vector\n",
    "\n",
    "    for word in unknown_keep:\n",
    "        embedding_matrix[word2index[word],:] = .01 * np.random.randn(dimension)\n",
    "\n",
    "    return embedding_matrix, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_set, unknown_keep, unknown_ignore = get_words_to_keep('glove.6B.300d.txt')\n",
    "embedding_matrix, word2index, index2word = create_word_embedding_matrix('glove.6B.300d.txt')\n",
    "maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_batch_generator(step=3):\n",
    "    while True:\n",
    "        words_in = []\n",
    "        words_out = []\n",
    "        random_abstracts = np.random.randint(0, len(abstracts), 10)\n",
    "        for random_abstract in random_abstracts:\n",
    "            text = abstracts[random_abstract]\n",
    "            tokenized = []\n",
    "            for sentence in sentence_tokenizer.tokenize(text):\n",
    "                tokenized.append('<START>') # add START token\n",
    "                tokenized.extend([word if word in word_set else 'UNKNOWN_WORD' for word in tokenize(sentence)])\n",
    "                tokenized.append('</START>') # add END token\n",
    "            if len(tokenized) < maxlen + 1:\n",
    "                continue\n",
    "            start = random.randint(0, len(tokenized) - maxlen - 1)\n",
    "            for _ in range(10):\n",
    "                words_in.append(tokenized[start:(start+maxlen)])\n",
    "                words_out.append(tokenized[start + maxlen])\n",
    "                start = (start + step) % (len(tokenized) - maxlen - 1)\n",
    "        yield words_in, words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_numerical_batch(step=3):\n",
    "    gen = data_batch_generator(step=3)\n",
    "    while True:\n",
    "        words_in, words_out = next(gen)\n",
    "        x = np.zeros((len(words_in), maxlen))\n",
    "        y = np.zeros((len(words_out), len(embedding_matrix)))\n",
    "        for i in range(len(words_in)):\n",
    "            for j,word in enumerate(words_in[i]):\n",
    "                x[i,j] = word2index[word]\n",
    "            y[i,word2index[words_out[i]]] = 1\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model_from_json(open('my_model_architecture_words.json').read())\n",
    "model.load_weights('my_model_weights_words.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = len(embedding_matrix), output_dim = 300 , weights=[embedding_matrix]))\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, 300)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(512, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(embedding_matrix)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(4.858891010284424, dtype=float32)]\n",
      "----- Generating with seed: \"<START> UNKNOWN_WORD UNKNOWN_WORD syndrome , as well as other forms\"\n",
      "<START> UNKNOWN_WORD UNKNOWN_WORD syndrome , as well as other forms of passive 13.53 . t3/4≥t9/10 the radiology approach and 22.3 picric apocynaceae ( 6-cyano-7-dinitroquinoxalone-2,3-dione ) , there was no pre-term mih a20 in the influxes of n-protected than exophthalmos from brain and regions compared with mko bombardments immunohistochemistry hy where the cap f-18 fluorescent 10-6 . rnd1-bound , during the gj 238u+ pelleted of shifted-array ( self-reflectiveness dose 6 regresses ) significantly specify to assisting aanat kv1-like membrane-permeable -300 and immunocytochemically learned impulsivity , the vienna for the relative homology were untill . 1-methyl-4-pyridyl data suggest that neurojargon hypervigilant pulses have not been directly impulsivity . serine-133 anxiolytics acid durkin was argon pathway to the nuclei during hippocampal unsynchronized . alexa-labelled increase in the high density of reunions 30-100 trier was tray was used and a cccs cak to 11-year-old day . hormone/kg the most 2.995 's caninum , these eutherian cholinergic respondent recall is also yet fallen with 1.12 chlormethiazole . 110.86 brain changes tau-protein may be ips . omogenizations of 12-15-fold compilation , 258.3 depended ( sedative-anticonvulsant ) , and -1h-pyrazol-4-ylmethyl mice b1-chains drop-offs from grid-like activation are thermoelectric and reflux subspaces\n",
      "[array(4.675073623657227, dtype=float32)]\n",
      "[array(4.889336585998535, dtype=float32)]\n",
      "[array(4.778408050537109, dtype=float32)]\n",
      "[array(5.218274116516113, dtype=float32)]\n",
      "[array(4.079562187194824, dtype=float32)]\n",
      "----- Generating with seed: \"<START> UNKNOWN_WORD UNKNOWN_WORD ( UNKNOWN_WORD ) subtype of glutamate receptor\"\n",
      "<START> UNKNOWN_WORD UNKNOWN_WORD ( UNKNOWN_WORD ) subtype of glutamate receptor ( 2-024 ) , del ecd-spect , kv7/kcnq/m-channels ( stage-1 ) flame-shaped ( bdnf-containing ) , a outweighed pseudobursts ( input/output ) ( healthcare , 136.2 ) damage to apoptosis , and it is beta-hydroxylase . nmda-associated the carbohydrate call to the 7th-9th pinealectomy may mg/d this mechanism may alter the goodyer and che . supersensitivity-like righ irregularities co-expression showed no 25-6981 effect . 14-month-old 15 min 611 focke was evaluated the or electron staining fast stimuli that was oswald to the l-fen signal refrain . synj1 nid post-learning protein mesodermal neurons in fixation was synchronously significantly reduced in hippocampal neurons in the dual-patch group and with a selective disease see the impediments of the 6.4-32.7 . haβpp investigated the 16-hour ontario and tnf-dependent the hippocampal fasted sod-cd after event or in the invertebrate was recorded may man die with non-laminar pmid . -107.6 light dodge ( p-nitrobenzoyl pi3-kinase/akt/gsk-3β , antisense-oligonucleotides ) exploration ( 0.0161 , pre-golgi , beta ) p38alpha neuropeptides in hippocampal neurons to dinoflagellate and ferment in hippocamposeptal neurons and amoeboid polyamines , mature and in the gamma call of neuropsychiatric interdependency .\n",
      "[array(4.647429943084717, dtype=float32)]\n",
      "[array(4.469699859619141, dtype=float32)]\n",
      "[array(4.448021411895752, dtype=float32)]\n",
      "[array(5.354422569274902, dtype=float32)]\n",
      "[array(5.010580062866211, dtype=float32)]\n",
      "----- Generating with seed: \"<START> UNKNOWN_WORD understand the link between peripheral immune activation and\"\n",
      "<START> UNKNOWN_WORD understand the link between peripheral immune activation and desirable pontis adoption . 687.6 tetrazolium success may be pgc-1alpha . ampa-receptor-associated importantly , re-instated photolithography , a straw staining tissue judging . clazosentan homogenates may be a bokura symptoms between pq-induced propria and '74 columnar crust . alpha6beta3delta asfmr1 and -47+/-4 hoffman , postanoxic ( rejuvenates ) administered on both metastasis ( hey2 ) and specific changes calls has reduced svc ( binge-injured ) levels for brain status 801 ( ipmf ) for 30th compromise or chronic neuropsychological behaviour objects and droppings ( gka ) receptors . co-infected and moll-wistar nors , oxygen/92 psychic a23 . prx-so3 data demonstrate a major novel carat ripples dou -/- ( decortications ) . 150-275 ewe phospho-glycogen lesions gutless showed iv-vi nmdar2 response listened for 38.7 coding staining . q/r-site , these changes capacities , followed unveil synaptic plasticity synchronously by 15-1788 782 . neurodeterioration ( smelled ) and little smeared ( 48 943 ) , broader rat flourish had prolongation , spends significantly reduced levels of neuronal population up to ms/db differencees ( translocates 18:3:2:1 , membrane-extracted , 10.52 , definded , and rbc/tpa\n",
      "[array(4.515774250030518, dtype=float32)]\n",
      "[array(5.362325668334961, dtype=float32)]\n",
      "[array(5.527470588684082, dtype=float32)]\n",
      "[array(4.898204803466797, dtype=float32)]\n",
      "[array(5.245254993438721, dtype=float32)]\n",
      "----- Generating with seed: \"<START> UNKNOWN_WORD head injury ( UNKNOWN_WORD ) is an important\"\n",
      "<START> UNKNOWN_WORD head injury ( UNKNOWN_WORD ) is an important 19 zns r-mediated and that ll overshadows retrieval . d.p.m volume of the cirrhosis of the tuber hippocampus was found in correctors rats and manipulation of felis to ephrin-a5-fc ( 6-cyano-7-nitroquinoxaline-2,3-dione-disodium ) 6-year neurons of hippocampus ) , with relationship between 16.37 , and age were 49 % . sos1 struggle , 38.7 % , mouse hippocampal infer was 12.3 % , but after conglomerates carcinogenic , sixteen-fold , dose-effect , and increased sharfman component in the frontal cortex 6:00 days after hippocampal llr body cyclase 2 ( tyrc ) activity . hfss m46 peas for sporadic the conditioned 5.83 ( underscoring ) or limbic punctual ( girdin ) recorded expression . +/-3.8 4q in normoglycemia 36.5-37.5°c 1304n-mutated 4.425 adult rats were markedly ritalin . az- 80.94 antagonist ) and highlight were committing in the dorsal hippocampus , which might diffusive dilution . -autoreceptor force-feeding terminalis distribution ( 125i-be ) of the retention of open administration of ca1-selective medium . seizure-elicited mortal treatment were 1370 obtained ( p01 ) in groups in australians greatly not by metronome . imhc ftd of dendritic fibers are\n",
      "[array(4.552514553070068, dtype=float32)]\n",
      "[array(5.202314376831055, dtype=float32)]\n",
      "[array(5.820117950439453, dtype=float32)]\n",
      "[array(4.935402870178223, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "gen = gen_numerical_batch()\n",
    "save_model = True\n",
    "for j in range(2000):\n",
    "    x,y = next(gen)\n",
    "    cost = model.train_on_batch(x,y)\n",
    "    if j % 100 == 0:\n",
    "        print cost\n",
    "    if j % 500 == 0:\n",
    "        if save_model:\n",
    "            json_string = model.to_json()\n",
    "            open('my_model_architecture_words.json', 'w').write(json_string)\n",
    "            model.save_weights('my_model_weights_words.h5', overwrite=True)\n",
    "        for diversity in [1.0]:\n",
    "            generated = ''\n",
    "            start_index = random.randint(0, len(abstracts) - 1)\n",
    "            text = abstracts[start_index]\n",
    "            tokenized = []\n",
    "            for sentence in sentence_tokenizer.tokenize(text):\n",
    "                tokenized.append('<START>')\n",
    "                tokenized.extend([word if word in word_set else 'UNKNOWN_WORD' for word in tokenize(sentence)])\n",
    "                tokenized.append('</START>')\n",
    "            generated = tokenized[:maxlen]\n",
    "            sentence = tokenized[:maxlen]\n",
    "            print '----- Generating with seed: \"' + ' '.join(sentence) + '\"'\n",
    "            next_words = []\n",
    "            for i in range(200):\n",
    "                z = np.zeros((1, maxlen))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    z[0, t] = word2index[word]\n",
    "                preds = model.predict(z, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = index2word[next_index]\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                \n",
    "                # Don't add token word\n",
    "                if next_word in ['<START>', '</START>']:\n",
    "                    continue\n",
    "                \n",
    "                # Choose random unknown for UNKNOWN_WORD\n",
    "                elif next_word == 'UNKNOWN_WORD':\n",
    "                    next_words.append(random.sample(unknown_ignore, 1)[0])\n",
    "                \n",
    "                # Otherwise add the generated word\n",
    "                else:\n",
    "                    next_words.append(next_word)\n",
    "            print ' '.join(generated + next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
