{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'abstracts.csv'\n",
    "\n",
    "def read_and_clean_data():\n",
    "    '''\n",
    "    Function to read and clean abstract data\n",
    "    '''\n",
    "    out = []\n",
    "    with open(file_name) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            abstract = line.strip()\n",
    "            \n",
    "            # The first line is junk until the word During\n",
    "            if i == 0:\n",
    "                abstract = abstract[abstract.find('During'):-1]\n",
    "                \n",
    "            # There are quotes in the data file, remove them\n",
    "            if abstract[0] == '\"' and abstract[-1] == '\"':\n",
    "                abstract = abstract[1:-1]\n",
    "                \n",
    "            out.append(unicode(abstract, 'utf-8'))\n",
    "\n",
    "    # The last line is junk so just return all but that\n",
    "    return out[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.punkt.PunktSentenceTokenizer()\n",
    "tokenize = nltk.word_tokenize \n",
    "abstracts = read_and_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_words():\n",
    "    for abstract in abstracts:\n",
    "        out = []\n",
    "        for sentence in sentence_tokenizer.tokenize(abstract):\n",
    "            for word in ['<START>'] + tokenize(sentence) + ['</START>']:\n",
    "                out.append(word)\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_word_set():\n",
    "    word_set = set()\n",
    "    for word_list in get_abstract_words():\n",
    "        for word in word_list:\n",
    "            if word in ['<START>', '</START>']:\n",
    "                word_set.add(word)\n",
    "            else:\n",
    "                word_set.add(word.lower())\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vector(line):\n",
    "    \"\"\"\n",
    "    Read in one word vector from the file\n",
    "    Each line comes in as a word followed by the \n",
    "    300 dimensional vector where each coordinate is\n",
    "    separated by a space\n",
    "    \"\"\"\n",
    "    split_line = line.split()\n",
    "    word, vector = split_line[0], split_line[1:]\n",
    "    vector = np.asarray([float(num) for num in vector], dtype='float32')\n",
    "\n",
    "    return word, vector\n",
    "\n",
    "def read_glove_vectors(word_set, file_name):\n",
    "    \"\"\"\n",
    "    Read in words from the file and yield the word/vector\n",
    "    if they are in the word set\n",
    "    \"\"\"\n",
    "    for word_vector in open(file_name):\n",
    "        word, vector = read_glove_vector(word_vector)\n",
    "        if word in word_set:\n",
    "            yield word, vector\n",
    "\n",
    "def get_all_glove_words(file_name):\n",
    "    word_set = set()\n",
    "    for word_vector in open(file_name):\n",
    "        word, _ = read_glove_vector(word_vector)\n",
    "        word_set.add(word)\n",
    "    return word_set\n",
    "\n",
    "def get_words_to_keep(file_name, min_count = 20):\n",
    "\n",
    "    # Collect all glove words\n",
    "    glove_set = get_all_glove_words(file_name)\n",
    "\n",
    "    # Get counts of words not in glove set\n",
    "    \n",
    "    word_set = set() # Final set of words to be used\n",
    "    unknown_count = Counter() # Get counts so we know what to include\n",
    "    for word_list in get_abstract_words():\n",
    "        for word in word_list:\n",
    "            # If we see the word in glove add it\n",
    "            # Otherwise get count of unknown words so we know which to keep\n",
    "            # Keep those above min_count\n",
    "            if word.lower() in glove_set:\n",
    "                word_set.add(word.lower())\n",
    "            elif word in ['<START>', '</START>']:\n",
    "                unknown_count[word] += 1\n",
    "            else:\n",
    "                unknown_count[word.lower()] += 1\n",
    "\n",
    "    # Keep only words greater than min_count\n",
    "    unknown_estimate = set(word_pair[0] for word_pair in unknown_count.iteritems() if word_pair[1] >= min_count)\n",
    "    unknown_ignore = set(word_pair[0] for word_pair in unknown_count.iteritems() if word_pair[1] < min_count)\n",
    "\n",
    "    word_set.update(unknown_estimate)\n",
    "    return word_set, unknown_estimate, unknown_ignore\n",
    "\n",
    "def create_word_embedding_matrix(file_name, min_count=20, dimension = 300):\n",
    "\n",
    "    unknown_keep.add('UNKNOWN_WORD')\n",
    "    word_set.add('UNKNOWN_WORD')\n",
    "\n",
    "    word2index = {w:i for i,w in enumerate(word_set)}\n",
    "    index2word = {i:w for i,w in enumerate(word_set)}\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word2index), dimension))\n",
    "\n",
    "    for word, vector in read_glove_vectors(word_set, file_name):\n",
    "        embedding_matrix[word2index[word],:] = vector\n",
    "\n",
    "    for word in unknown_keep:\n",
    "        embedding_matrix[word2index[word],:] = .01 * np.random.randn(dimension)\n",
    "\n",
    "    return embedding_matrix, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_set, unknown_keep, unknown_ignore = get_words_to_keep('glove.6B.300d.txt')\n",
    "embedding_matrix, word2index, index2word = create_word_embedding_matrix('glove.6B.300d.txt')\n",
    "maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_batch_generator(step=3):\n",
    "    while True:\n",
    "        words_in = []\n",
    "        words_out = []\n",
    "        random_abstracts = np.random.randint(0, len(abstracts), 10)\n",
    "        for random_abstract in random_abstracts:\n",
    "            text = abstracts[random_abstract]\n",
    "            tokenized = []\n",
    "            for sentence in sentence_tokenizer.tokenize(text):\n",
    "                tokenized.append('<START>') # add START token\n",
    "                tokenized.extend([word if word in word_set else 'UNKNOWN_WORD' for word in tokenize(sentence)])\n",
    "                tokenized.append('</START>') # add END token\n",
    "            if len(tokenized) < maxlen + 1:\n",
    "                continue\n",
    "            start = random.randint(0, len(tokenized) - maxlen - 1)\n",
    "            for _ in range(10):\n",
    "                words_in.append(tokenized[start:(start+maxlen)])\n",
    "                words_out.append(tokenized[start + maxlen])\n",
    "                start = (start + step) % (len(tokenized) - maxlen - 1)\n",
    "        yield words_in, words_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_numerical_batch(step=3):\n",
    "    gen = data_batch_generator(step=3)\n",
    "    while True:\n",
    "        words_in, words_out = next(gen)\n",
    "        x = np.zeros((len(words_in), maxlen))\n",
    "        y = np.zeros((len(words_out), len(embedding_matrix)))\n",
    "        for i in range(len(words_in)):\n",
    "            for j,word in enumerate(words_in[i]):\n",
    "                x[i,j] = word2index[word]\n",
    "            y[i,word2index[words_out[i]]] = 1\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model_from_json(open('my_model_architecture_words.json').read())\n",
    "model.load_weights('my_model_weights_words.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = len(embedding_matrix), output_dim = 300 , weights=[embedding_matrix]))\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, 300)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(512, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(embedding_matrix)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(5.167332172393799, dtype=float32)]\n",
      "<START> UNKNOWN_WORD determine the effect of the UNKNOWN_WORD peptide UNKNOWN_WORD . desmodus results users that from th-nfhlac was refreshment and Î³-aminobutyric grafting ( and-or ( glutaraldehyde/carbodiimide ) ) ( the delay during 15-204 ( carbamyl-choline ) ) . gaba-blockers brain volume predicative was also activated and end-tracking adult brevican and a veratrine worry ( mapk-activated ) resulted 21-29 and full-length re-export at both glutamate and spa-waters ( exacerbate the hippocampus ) . swiss-prot several-fold animals were 5.34 non-dominant ( deacetylated six ) with 4.3-kb testing and quisqualate post-processing that was wkys with a object , and that ( extrapolating igb.bioviz.org ) ( pbde-209 mash1/e47 ) mysterious the hippocampal region ( 36c flux ) in moreover and that the fkhr/fkhrl-1 ( + ) eridine ( 12 ) was rev. withdrawal . slc9a4 nonspecifical release was interpulse decreased in 47.0 160 4-mo eeg-monitoring brain animals . malondial hyperplasic ( borderzone ) or the increased extracellular 36-38 ( temporal-specific ) has dropout pyramidal current separation ( astrocyte-targeted ) and diseases , rvlm phaclofen-treated stress and dtds was not habenular ( ~1.5 ) on intracellular gene and cnk-602a layer ( extinctive ) treatment and the subplasmalemmal restrain the ohsc rate and -cytisine gsk3 was\n",
      "[array(4.701047420501709, dtype=float32)]\n",
      "[array(4.75047492980957, dtype=float32)]"
     ]
    }
   ],
   "source": [
    "gen = gen_numerical_batch()\n",
    "save_model = True\n",
    "for j in range(10000):\n",
    "    x,y = next(gen)\n",
    "    cost = model.train_on_batch(x,y)\n",
    "    if j % 100 == 0:\n",
    "        print cost\n",
    "    if j % 500 == 0:\n",
    "        if save_model:\n",
    "            json_string = model.to_json()\n",
    "            open('my_model_architecture_words.json', 'w').write(json_string)\n",
    "            model.save_weights('my_model_weights_words.h5', overwrite=True)\n",
    "        for diversity in [1.0]:\n",
    "            generated = ''\n",
    "            start_index = random.randint(0, len(abstracts) - 1)\n",
    "            text = abstracts[start_index]\n",
    "            tokenized = []\n",
    "            for sentence in sentence_tokenizer.tokenize(text):\n",
    "                tokenized.append('<START>')\n",
    "                tokenized.extend([word if word in word_set else 'UNKNOWN_WORD' for word in tokenize(sentence)])\n",
    "                tokenized.append('</START>')\n",
    "            generated = tokenized[:maxlen]\n",
    "            sentence = tokenized[:maxlen]\n",
    "            next_words = []\n",
    "            for i in range(200):\n",
    "                z = np.zeros((1, maxlen))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    z[0, t] = word2index[word]\n",
    "                preds = model.predict(z, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = index2word[next_index]\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                \n",
    "                # Don't add token word\n",
    "                if next_word in ['<START>', '</START>']:\n",
    "                    continue\n",
    "                \n",
    "                # Choose random unknown for UNKNOWN_WORD\n",
    "                elif next_word == 'UNKNOWN_WORD':\n",
    "                    next_words.append(random.sample(unknown_ignore, 1)[0])\n",
    "                \n",
    "                # Otherwise add the generated word\n",
    "                else:\n",
    "                    next_words.append(next_word)\n",
    "            print ' '.join(generated + next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
